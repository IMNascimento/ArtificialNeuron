def neuron_main():
    from learning.neuron import Neuron

    # ==========================================
    # EXEMPLO 1: NEURÃ”NIO APRENDE A FAZER CONTA
    # ==========================================

    print("=" * 70)
    print("ğŸ§® EXEMPLO 1: ENSINANDO UM NEURÃ”NIO A MULTIPLICAR POR 2")
    print("=" * 70)
    print("\nVamos ensinar o neurÃ´nio que: NÃšMERO Ã— 2 = RESULTADO\n")

    # Create a single-input neuron with LINEAR output (regression)
    neuron_math = Neuron(num_inputs=1, activation="linear")

    # Training samples: learn y = 2x
    training_data = [
        ([0.1], 0.2),
        ([0.2], 0.4),
        ([0.3], 0.6),
        ([0.4], 0.8),
        ([0.5], 1.0),
    ]

    # Before learning
    print("âŒ ANTES DE APRENDER (neurÃ´nio chutando):")
    for x, y_true in training_data:
        y_pred = neuron_math.think(x)
        print(f"   {x[0]} Ã— 2 = {y_pred:.3f}  (correto seria: {y_true})")

    # Train (linear output converges rÃ¡pido; 100-1000 jÃ¡ resolve)
    neuron_math.train(training_data, epochs=100, learning_rate=0.05)

    # After learning
    print("âœ… DEPOIS DE APRENDER:")
    for x, y_true in training_data:
        y_pred = neuron_math.think(x)
        print(f"   {x[0]} Ã— 2 = {y_pred:.3f}  (correto: {y_true})")

    # Show learned parameters
    print("\nğŸ” ParÃ¢metros aprendidos (aprox.):")
    print(f"   w â‰ˆ {neuron_math.weights[0]:.4f}, b â‰ˆ {neuron_math.bias:.4f}")

    # Test with unseen numbers
    print("\nğŸ¯ TESTANDO COM NÃšMEROS NOVOS (que nÃ£o estavam no treino):")
    for x_new in ([0.35], [0.7], [0.9], [1.5], [2.0], [0.11]):
        y_pred = neuron_math.think(x_new)
        print(f"   {x_new[0]} Ã— 2 = {y_pred:.3f}  (correto seria: {x_new[0] * 2})")
    print("   ğŸ‘€ Viu? Agora ele GENERALIZA linearmente (sem saturar).")


def neural_network_main():
    from learning.neural_network import NeuralNetwork
    # ==========================================
    # EXEMPLO 2: RECONHECER LETRA "T" vs "X"
    # ==========================================

    print("\n" + "=" * 70)
    print("âœï¸  EXEMPLO 2: REDE DE 10 NEURÃ”NIOS RECONHECENDO LETRAS")
    print("=" * 70)
    print("\nVamos ensinar a rede a reconhecer padrÃµes visuais simples!")
    print("Cada letra Ã© representada por 9 pixels (grade 3Ã—3)\n")

    # Criar rede: 9 entradas (3Ã—3 pixels) â†’ 10 neurÃ´nios â†’ 2 saÃ­das (T ou X)
    rede = NeuralNetwork(num_inputs=9, num_hidden=10, num_outputs=2, hidden_activation="sigmoid", output_activation="sigmoid")

    # RepresentaÃ§Ã£o visual das letras em grade 3Ã—3
    print("ğŸ“ PADRÃ•ES QUE VAMOS ENSINAR:")
    print("\nLetra T:        Letra X:")
    print("â–ˆ â–ˆ â–ˆ          â–ˆ Â· â–ˆ")
    print("Â· â–ˆ Â·          Â· â–ˆ Â·")
    print("Â· â–ˆ Â·          â–ˆ Â· â–ˆ")
    print()

    # Dados de treino
    # 1 = pixel aceso, 0 = pixel apagado
    # SaÃ­da: [1, 0] = Ã© um T, [0, 1] = Ã© um X

    dados_letras = [
        # Letra T (vÃ¡rias variaÃ§Ãµes)
        ([1,1,1, 0,1,0, 0,1,0], [1, 0]),  # T perfeito
        ([1,1,1, 0,1,0, 0,1,1], [1, 0]),  # T com ruÃ­do
        ([1,1,0, 0,1,0, 0,1,0], [1, 0]),  # T incompleto
        
        # Letra X (vÃ¡rias variaÃ§Ãµes)  
        ([1,0,1, 0,1,0, 1,0,1], [0, 1]),  # X perfeito
        ([1,0,1, 0,1,0, 1,0,0], [0, 1]),  # X com ruÃ­do
        ([1,0,1, 1,1,0, 1,0,1], [0, 1]),  # X com ruÃ­do
    ]

    # ANTES do treino
    print("âŒ ANTES DO TREINO (rede chutando):")
    for i, (entrada, esperado) in enumerate(dados_letras):
        saidas, _ = rede.think(entrada)
        letra = "T" if esperado[0] > esperado[1] else "X"
        prob_t = saidas[0] * 100
        prob_x = saidas[1] * 100
        print(f"   PadrÃ£o {i+1} (Ã© um {letra}): T={prob_t:.1f}% | X={prob_x:.1f}%")

    # TREINANDO
    rede.train(dados_letras, epochs=20000)

    # DEPOIS do treino
    print("âœ… DEPOIS DO TREINO:")
    for i, (entrada, esperado) in enumerate(dados_letras):
        saidas, _ = rede.think(entrada)
        letra = "T" if esperado[0] > esperado[1] else "X"
        prob_t = saidas[0] * 100
        prob_x = saidas[1] * 100
        print(f"   PadrÃ£o {i+1} (Ã© um {letra}): T={prob_t:.1f}% | X={prob_x:.1f}%")

    # TESTAR com padrÃ£o totalmente NOVO
    print("\nğŸ¯ TESTANDO COM PADRÃƒO NOVO (nunca visto no treino):")
    print("\nNovo T:         Novo X:")
    print("â–ˆ â–ˆ Â·          Â· Â· â–ˆ")
    print("Â· â–ˆ Â·          Â· â–ˆ Â·")
    print("Â· â–ˆ Â·          â–ˆ Â· Â·")

    novo_t = [1,1,0, 0,1,0, 0,1,0]
    novo_x = [0,0,1, 0,1,0, 1,0,0]

    saidas_t, _ = rede.think(novo_t)
    saidas_x, _ = rede.think(novo_x)

    print(f"\nNovo T: T={saidas_t[0]*100:.1f}% | X={saidas_t[1]*100:.1f}%")
    print(f"Novo X: T={saidas_x[0]*100:.1f}% | X={saidas_x[1]*100:.1f}%")
    print("\nğŸ‘ A rede GENERALIZOU! Reconhece padrÃµes que nunca viu!")

def generative_network_main():
    from learning.generative_network import GenerativeNetwork

    # =========================
    # helpers (sampling etc.)
    # =========================
    import math
    import random

    def temperature_sample(probs, temperature: float = 0.8, top_k: int | None = 5):
        """Amostragem com temperatura + top-k.
        probs: lista de probabilidades (somam ~1)."""
        # evita zeros
        eps = 1e-12
        probs = [max(p, eps) for p in probs]

        # volta para logits, aplica temperatura
        logits = [math.log(p) for p in probs]
        logits = [z / max(temperature, 1e-6) for z in logits]

        # top-k (opcional)
        if top_k is not None and top_k < len(logits):
            # mantÃ©m Ã­ndices dos top_k logits
            top_idx = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:top_k]
            mask = [0.0] * len(logits)
            for i in top_idx:
                mask[i] = 1.0
            logits = [logits[i] if mask[i] > 0 else float("-inf") for i in range(len(logits))]

        # softmax estÃ¡vel
        m = max(logits)
        exps = [0.0 if (z == float("-inf")) else math.exp(z - m) for z in logits]
        s = sum(exps) or 1.0
        new_probs = [e / s for e in exps]

        # roleta
        r = random.random()
        acc = 0.0
        for i, p in enumerate(new_probs):
            acc += p
            if r <= acc:
                return i
        return len(new_probs) - 1  # fallback

    def char_to_onehot(ch, char_to_idx, vocab_size: int):
        """Converte letra em vetor (one-hot encoding)"""
        v = [0.0] * vocab_size
        if ch in char_to_idx:
            v[char_to_idx[ch]] = 1.0
        return v

    def topk_indices(probs, k=3):
        return sorted(range(len(probs)), key=lambda i: probs[i], reverse=True)[:k]

    # ==========================================
    # EXEMPLO 1: IA QUE COMPLETA PALAVRAS
    # ==========================================

    print("=" * 80)
    print("ğŸ’¬ EXEMPLO 1: IA QUE APRENDE A COMPLETAR SAUDAÃ‡Ã•ES")
    print("=" * 80)
    print("\nVamos ensinar a IA a entender padrÃµes de saudaÃ§Ãµes em portuguÃªs!\n")

    # VocabulÃ¡rio (AtenÃ§Ã£o: tudo UPPER + espaÃ§o)
    vocabulary = ['B', 'O', 'M', 'D', 'I', 'A', 'T', 'R', 'E', 'N', ' ']
    char_to_idx = {ch: i for i, ch in enumerate(vocabulary)}
    idx_to_char = {i: ch for ch, i in char_to_idx.items()}

    print(f"ğŸ“– VocabulÃ¡rio da IA: {vocabulary}")
    print(f"   Total: {len(vocabulary)} caracteres\n")

    # Rede com 10 neurÃ´nios ocultos
    net = GenerativeNetwork(vocab_size=len(vocabulary), num_hidden=10)

    # ALIASES para manter compatibilidade com seu cÃ³digo antigo
    net.gerar_proximo = net.predict_next
    net.gerar_texto = net.generate_text

    print("ğŸ“ Ensinando padrÃµes:")
    train_pairs_greetings = [
        # BOM DIA
        ('B', 'O'), ('O', 'M'), ('M', ' '), (' ', 'D'), ('D', 'I'), ('I', 'A'),
        # BOA TARDE
        ('B', 'O'), ('O', 'A'), ('A', ' '), (' ', 'T'), ('T', 'A'), ('A', 'R'), ('R', 'D'), ('D', 'E'),
        # BOA NOITE
        ('B', 'O'), ('O', 'A'), ('A', ' '), (' ', 'N'), ('N', 'O'), ('O', 'I'), ('I', 'T'), ('T', 'E'),
    ]
    print("   â€¢ B â†’ O (inÃ­cio de BOA/BOM)")
    print("   â€¢ O â†’ M (BOM) ou O â†’ A (BOA)")
    print("   â€¢ M â†’ ' ' (BOM_)")
    print("   â€¢ ' ' â†’ D/T/N (inÃ­cio de DIA/TARDE/NOITE)")
    print("   â€¢ E assim por diante...\n")

    # montar dados (x_onehot, y_onehot)
    train_data = []
    for a, b in train_pairs_greetings:
        x = char_to_onehot(a, char_to_idx, len(vocabulary))
        y = char_to_onehot(b, char_to_idx, len(vocabulary))
        train_data.append((x, y))

    # Treino
    net.train(train_data, epochs=5000, learning_rate=0.2, report_every=500)

    # TESTE
    print("ğŸ§ª TESTANDO O QUE A IA APRENDEU:")
    print("-" * 80)
    test_chars = ['B', 'O', 'M', ' ', 'D', 'A', 'T', 'N']
    for ch in test_chars:
        x = char_to_onehot(ch, char_to_idx, len(vocabulary))
        probs = net.predict_next(x)
        top_idx = topk_indices(probs, k=3)
        print(f"\n   Se vÃª '{ch}', a IA acha que o prÃ³ximo pode ser:")
        for i in top_idx:
            print(f"      '{idx_to_char[i]}' com {probs[i]*100:.1f}% de certeza")

    # ==========================================
    # EXEMPLO 2: IA GERANDO TEXTO COMPLETO
    # ==========================================

    print("\n\n" + "=" * 80)
    print("âœ¨ EXEMPLO 2: IA GERANDO TEXTO AUTOMATICAMENTE")
    print("=" * 80)
    print("\nAgora a IA vai GERAR texto sozinha, letra por letra!")
    print("Como? Ela vÃª uma letra, prevÃª a prÃ³xima, adiciona, e repete!\n")

    print("ğŸ¯ GERAÃ‡Ã•ES AUTOMÃTICAS (com temperature/top-k):")
    print("-" * 80)

    # geraÃ§Ã£o com sampling (em vez de greedy)
    def generate_text_sampled(seed: str, length: int = 10, temperature: float = 0.8, top_k: int | None = 5):
        """Gera texto usando sampling com temperatura e top-k."""
        out = seed
        while len(out) < len(seed) + length:
            last = out[-1]
            if last not in char_to_idx:
                break
            x = char_to_onehot(last, char_to_idx, len(vocabulary))
            probs = net.predict_next(x)
            next_i = temperature_sample(probs, temperature=temperature, top_k=top_k)
            out += idx_to_char[next_i]
        return out

    seeds = ['B', 'BO', 'BOM']
    for s in seeds:
        txt = generate_text_sampled(s, length=10, temperature=0.8, top_k=3)
        print(f"   ComeÃ§ando com '{s}' â†’ Gerou: '{txt}'")

    # ==========================================
    # EXEMPLO 3: IA MAIS AVANÃ‡ADA - NOMES
    # ==========================================

    print("\n\n" + "=" * 80)
    print("ğŸš€ EXEMPLO 3: IA GERANDO NOMES BRASILEIROS")
    print("=" * 80)

    vocab_names = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ ')
    char_to_idx2 = {ch: i for i, ch in enumerate(vocab_names)}
    idx_to_char2 = {i: ch for ch, i in char_to_idx2.items()}

    print(f"\nğŸ“– Novo vocabulÃ¡rio: {len(vocab_names)} letras (A-Z + espaÃ§o)")

    net_names = GenerativeNetwork(vocab_size=len(vocab_names), num_hidden=20)
    # aliases
    net_names.gerar_proximo = net_names.predict_next
    net_names.gerar_texto = net_names.generate_text

    names_train = ["MARIA", "JOAO", "JOSE", "ANA", "PEDRO",
                   "CARLOS", "PAULO", "LUCAS", "JULIA", "BEATRIZ"]
    print(f"\nğŸ“š Ensinando {len(names_train)} nomes brasileiros:")
    print(f"   {', '.join(names_train[:5])}, ...")

    # pares consecutivos
    train_names = []
    for name in names_train:
        for i in range(len(name) - 1):
            x = char_to_onehot(name[i], char_to_idx2, len(vocab_names))
            y = char_to_onehot(name[i+1], char_to_idx2, len(vocab_names))
            train_names.append((x, y))

    # pequeno boost nos pares com espaÃ§o final para â€œfecharâ€ nomes
    # (ex.: "ANA " -> aumenta chance de espaÃ§o depois de A)
    for name in names_train:
        x = char_to_onehot(name[-1], char_to_idx2, len(vocab_names))
        y = char_to_onehot(' ', char_to_idx2, len(vocab_names))
        train_names.append((x, y))

    net_names.train(train_names, epochs=9000, learning_rate=0.25, report_every=1000)

    print("\nâœ¨ IA GERANDO NOMES NOVOS (sampling com temperature/top-k):")
    print("-" * 80)

    def generate_name_sampled(seed: str, length: int = 8, temperature: float = 0.9, top_k: int | None = 5):
        out = seed
        while len(out) < len(seed) + length:
            last = out[-1]
            if last not in char_to_idx2:
                break
            x = char_to_onehot(last, char_to_idx2, len(vocab_names))
            probs = net_names.predict_next(x)
            next_i = temperature_sample(probs, temperature=temperature, top_k=top_k)
            out += idx_to_char2[next_i]
        return out

    initials = ['M', 'J', 'P', 'L', 'C']
    for ini in initials:
        gen = generate_name_sampled(ini, length=8, temperature=0.9, top_k=5)
        print(f"   ComeÃ§ando com '{ini}' â†’ Nome gerado: '{gen}'")

def generative_learning():
    # ==========================================
    # EXPLICAÃ‡ÃƒO FINAL
    # ==========================================

    print("\n\n" + "=" * 80)
    print("ğŸ“ COMO FUNCIONA A IA GENERATIVA (RESUMO PARA AULA)")
    print("=" * 80)
    print("""
    1. ğŸ§  APRENDIZADO DE PADRÃ•ES:
    â€¢ A rede vÃª MILHARES de exemplos: "depois de B vem O", "depois de O vem A"...
    â€¢ Ajusta seus pesos (memÃ³ria) para memorizar esses padrÃµes
    â€¢ Como crianÃ§a aprendendo a escrever vendo muitos exemplos!

    2. âœ¨ GERAÃ‡ÃƒO CRIATIVA:
    â€¢ VocÃª dÃ¡ um INÃCIO (ex: "B")
    â€¢ A IA prevÃª: "depois de B, provavelmente vem O"
    â€¢ Adiciona "O" ao texto
    â€¢ Agora tem "BO", prevÃª o prÃ³ximo...
    â€¢ Repete esse processo letra por letra!

    3. ğŸ¯ POR QUE FUNCIONA:
    â€¢ NÃ£o Ã© mÃ¡gica! Ã‰ ESTATÃSTICA APRENDIDA
    â€¢ A rede aprendeu probabilidades: "apÃ³s M geralmente vem espaÃ§o"
    â€¢ Quanto mais dados, melhor ela aprende padrÃµes
    â€¢ ChatGPT faz o MESMO, mas com BILHÃ•ES de neurÃ´nios!

    4. ğŸ’¡ ANALOGIA COM CÃ‰REBRO:
    â€¢ Quando vocÃª escreve, seu cÃ©rebro prevÃª a prÃ³xima palavra
    â€¢ VocÃª aprendeu padrÃµes lendo milhÃµes de palavras na vida
    â€¢ IA faz igual: aprende padrÃµes e os usa para criar texto novo
    â€¢ DiferenÃ§a: cÃ©rebro tem 86 bilhÃµes de neurÃ´nios!

    5. ğŸš€ IA MODERNA (ChatGPT, etc):
    â€¢ Mesma ideia bÃ¡sica que mostramos aqui
    â€¢ Mas com arquitetura mais complexa (Transformers)
    â€¢ BILHÃ•ES de neurÃ´nios
    â€¢ Treinada em TODO texto da internet
    â€¢ Por isso consegue conversar, programar, criar histÃ³rias...
    
    6. âš¡ LIMITAÃ‡Ã•ES DESTE EXEMPLO:
    â€¢ Nossa rede Ã© PEQUENA (10-15 neurÃ´nios)
    â€¢ ChatGPT tem 175 BILHÃ•ES de parÃ¢metros!
    â€¢ Mas o PRINCÃPIO Ã© exatamente o mesmo!
    """)

def learning():
     # ==========================================
    # RESUMO EDUCACIONAL
    # ==========================================

    print("\n" + "=" * 70)
    print("ğŸ“ O QUE APRENDEMOS:")
    print("=" * 70)
    print("""
    1. ğŸ§  UM NEURÃ”NIO SOZINHO:
    â€¢ Aprendeu a fazer conta (multiplicar por 2)
    â€¢ Ajustou sua "memÃ³ria" (pesos) atravÃ©s dos erros
    â€¢ Conseguiu fazer contas com nÃºmeros novos!
    
    2. ğŸ‘¥ EQUIPE DE 10 NEURÃ”NIOS:
    â€¢ Trabalharam juntos para reconhecer padrÃµes complexos
    â€¢ Cada neurÃ´nio da camada oculta detecta caracterÃ­sticas diferentes
    â€¢ Juntos, tomam decisÃµes mais inteligentes
    
    3. ğŸš€ PODER DA GENERALIZAÃ‡ÃƒO:
    â€¢ NÃ£o decoram respostas, aprendem PADRÃ•ES
    â€¢ Funcionam com dados que nunca viram antes
    â€¢ Como humanos: aprendemos por exemplos e aplicamos em situaÃ§Ãµes novas!

    4. ğŸ’¡ ANALOGIA COM O CÃ‰REBRO:
    â€¢ NeurÃ´nios biolÃ³gicos aprendem fortalecendo conexÃµes Ãºteis
    â€¢ NeurÃ´nios artificiais ajustam "pesos" (forÃ§a das conexÃµes)
    â€¢ Ambos aprendem com repetiÃ§Ã£o e correÃ§Ã£o de erros
    â€¢ Trabalho em equipe gera inteligÃªncia complexa!
    """)


if __name__ == "__main__":
    neuron_main()
    #neural_network_main()
    #learning()
    #generative_network_main()
    #generative_learning()


    
   